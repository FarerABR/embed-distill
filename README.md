# embed-distill

A knowledge distillation project that trains a lightweight Persian BERT model (`bert-fa-base-uncased`) to replicate the embedding quality of the larger `jina-embeddings-v3` teacher model on Persian text. The training data is drawn from Persian Wikipedia sentences.
